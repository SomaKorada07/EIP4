<H1>Log for 20 epochs</H1>

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

Train on 60000 samples, validate on 10000 samples
Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.0025.
60000/60000 [==============================] - 13s 222us/step - loss: 0.2684 - acc: 0.9127 - val_loss: 0.0643 - val_acc: 0.9793
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0018953753.
60000/60000 [==============================] - 7s 111us/step - loss: 0.0732 - acc: 0.9773 - val_loss: 0.0446 - val_acc: 0.9854
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0015262515.
60000/60000 [==============================] - 7s 111us/step - loss: 0.0560 - acc: 0.9823 - val_loss: 0.0352 - val_acc: 0.9881
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.0012774655.
60000/60000 [==============================] - 7s 112us/step - loss: 0.0492 - acc: 0.9845 - val_loss: 0.0317 - val_acc: 0.9887
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0010984183.
60000/60000 [==============================] - 7s 111us/step - loss: 0.0436 - acc: 0.9861 - val_loss: 0.0272 - val_acc: 0.9910
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0009633911.
60000/60000 [==============================] - 7s 112us/step - loss: 0.0405 - acc: 0.9867 - val_loss: 0.0285 - val_acc: 0.9912
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.0008579272.
60000/60000 [==============================] - 7s 109us/step - loss: 0.0370 - acc: 0.9884 - val_loss: 0.0257 - val_acc: 0.9907
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0007732756.
60000/60000 [==============================] - 7s 110us/step - loss: 0.0353 - acc: 0.9884 - val_loss: 0.0254 - val_acc: 0.9909
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.0007038288.
60000/60000 [==============================] - 7s 111us/step - loss: 0.0331 - acc: 0.9895 - val_loss: 0.0241 - val_acc: 0.9922
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.000645828.
60000/60000 [==============================] - 6s 108us/step - loss: 0.0310 - acc: 0.9904 - val_loss: 0.0234 - val_acc: 0.9926
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.0005966587.
60000/60000 [==============================] - 7s 109us/step - loss: 0.0299 - acc: 0.9903 - val_loss: 0.0222 - val_acc: 0.9929
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.0005544467.
60000/60000 [==============================] - 7s 112us/step - loss: 0.0280 - acc: 0.9911 - val_loss: 0.0206 - val_acc: 0.9936
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.0005178128.
60000/60000 [==============================] - 6s 108us/step - loss: 0.0281 - acc: 0.9905 - val_loss: 0.0221 - val_acc: 0.9929
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.0004857198.
60000/60000 [==============================] - 7s 113us/step - loss: 0.0271 - acc: 0.9909 - val_loss: 0.0203 - val_acc: 0.9931
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0004573729.
60000/60000 [==============================] - 7s 114us/step - loss: 0.0265 - acc: 0.9915 - val_loss: 0.0203 - val_acc: 0.9940
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0004321521.
60000/60000 [==============================] - 7s 110us/step - loss: 0.0259 - acc: 0.9918 - val_loss: 0.0198 - val_acc: 0.9935
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.0004095675.
60000/60000 [==============================] - 6s 108us/step - loss: 0.0240 - acc: 0.9918 - val_loss: 0.0186 - val_acc: 0.9942
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0003892262.
60000/60000 [==============================] - 7s 112us/step - loss: 0.0242 - acc: 0.9919 - val_loss: 0.0180 - val_acc: 0.9943
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0003708098.
60000/60000 [==============================] - 6s 107us/step - loss: 0.0235 - acc: 0.9924 - val_loss: 0.0191 - **val_acc: 0.9947**
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.0003540575.
60000/60000 [==============================] - 7s 113us/step - loss: 0.0241 - acc: 0.9921 - val_loss: 0.0196 - val_acc: 0.9942
<keras.callbacks.History at 0x7f60f9cebac8>

<H1> model.evaluate() result </H1>

[0.019615218751045178, 0.9942]

<H1>Strategy taken to achieve the above results</H1>

- Followed cake architecture (increased kernels and then used transition block to reduce the kernels)
- Tuned the hyperparameters - Dropout rate and also initial Learning Rate
- Implemented LearningRateScheduler to change LR for every epoch
- Tried to reach global RF close to image size
